{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries and initializing Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             columns|\n",
      "+--------------------+\n",
      "|[0000047951107191...|\n",
      "|[0000047951107191...|\n",
      "|[0000716910049411...|\n",
      "|[0000716910049411...|\n",
      "|[0000716910049411...|\n",
      "|[0001101779879861...|\n",
      "|[0001101779879861...|\n",
      "|[0001102065933811...|\n",
      "|[0001102848156801...|\n",
      "|[0001103278564691...|\n",
      "|[0001103399354531...|\n",
      "|[0001103399354531...|\n",
      "|[0001103399354531...|\n",
      "|[0001103399354531...|\n",
      "|[0001103438685671...|\n",
      "|[0001103438685671...|\n",
      "|[0001103594243391...|\n",
      "|[0001103594243391...|\n",
      "|[0001104154004291...|\n",
      "|[0001104154004291...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your .dat file\n",
    "file_path = \"data/dec17pub.dat\"\n",
    "\n",
    "# Read the .dat file as a DataFrame\n",
    "raw_data = spark.read.text(file_path)\n",
    "\n",
    "# Split the lines using a delimiter (modify as needed)\n",
    "delimiter = \"\\t\"  # Example delimiter\n",
    "split_columns = split(raw_data.value, delimiter).alias(\"columns\")\n",
    "\n",
    "# Select and process the columns from the split data\n",
    "processed_data = raw_data.select(split_columns)\n",
    "\n",
    "# Show the processed data (you can perform further transformations)\n",
    "processed_data.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m col\n\u001b[1;32m----> 3\u001b[0m first_element_data \u001b[39m=\u001b[39m processed_data\u001b[39m.\u001b[39mselect(col(\u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mfirst_element\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m first_element_data\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\emmau\\DocumentsC\\Users\\emmau\\Documents\\lib\\site-packages\\pyspark\\sql\\utils.py:160\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(functions, f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\emmau\\DocumentsC\\Users\\emmau\\Documents\\lib\\site-packages\\pyspark\\sql\\functions.py:221\u001b[0m, in \u001b[0;36mcol\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m@try_remote_functions\u001b[39m\n\u001b[0;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcol\u001b[39m(col: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Column:\n\u001b[0;32m    196\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m    Returns a :class:`~pyspark.sql.Column` based on the given column name.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m    Column<'x'>\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m     \u001b[39mreturn\u001b[39;00m _invoke_function(\u001b[39m\"\u001b[39;49m\u001b[39mcol\u001b[39;49m\u001b[39m\"\u001b[39;49m, col)\n",
      "File \u001b[1;32mc:\\Users\\emmau\\DocumentsC\\Users\\emmau\\Documents\\lib\\site-packages\\pyspark\\sql\\functions.py:93\u001b[0m, in \u001b[0;36m_invoke_function\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_invoke_function\u001b[39m(name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39margs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Column:\n\u001b[0;32m     89\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39m    Invokes JVM function identified by name with args\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39m    and wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     jf \u001b[39m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[39m.\u001b[39m_active_spark_context)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m Column(jf(\u001b[39m*\u001b[39margs))\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "first_element_data = processed_data.select(col(\"columns\")[0].alias(\"first_element\"))\n",
    "first_element_data.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
