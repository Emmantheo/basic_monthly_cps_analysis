{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30793a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries and initializing Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1887ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             columns|\n",
      "+--------------------+\n",
      "|[0000047951107191...|\n",
      "|[0000047951107191...|\n",
      "|[0000716910049411...|\n",
      "|[0000716910049411...|\n",
      "|[0000716910049411...|\n",
      "|[0001101779879861...|\n",
      "|[0001101779879861...|\n",
      "|[0001102065933811...|\n",
      "|[0001102848156801...|\n",
      "|[0001103278564691...|\n",
      "|[0001103399354531...|\n",
      "|[0001103399354531...|\n",
      "|[0001103399354531...|\n",
      "|[0001103399354531...|\n",
      "|[0001103438685671...|\n",
      "|[0001103438685671...|\n",
      "|[0001103594243391...|\n",
      "|[0001103594243391...|\n",
      "|[0001104154004291...|\n",
      "|[0001104154004291...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your .dat file\n",
    "file_path = \"dec17pub.dat\"\n",
    "\n",
    "# Read the .dat file as a DataFrame\n",
    "raw_data = spark.read.text(file_path)\n",
    "\n",
    "# Split the lines using a delimiter (modify as needed)\n",
    "delimiter = \"\\t\"  # Example delimiter\n",
    "split_columns = split(raw_data.value, delimiter).alias(\"columns\")\n",
    "\n",
    "# Select and process the columns from the split data\n",
    "processed_data = raw_data.select(split_columns)\n",
    "\n",
    "# Show the processed data (you can perform further transformations)\n",
    "processed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0055499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 146456\n"
     ]
    }
   ],
   "source": [
    "#checking number of records\n",
    "\n",
    "num_rows = processed_data.count()\n",
    "print(\"Number of records:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036cd6c6",
   "metadata": {},
   "source": [
    "There are 146,456 records in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcae055f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[hs_ID: string, month: int, year: int, fin_out: string, hs_unt: int, hs_tel: int, else_tel: int, tel_intv: int, fam_inc: int, hs_type: int, intv_type: int, REG: int, DIV: string, race: string]\n",
      "+---------------+-----+----+-------+------+------+--------+--------+-------+-------+---------+---+---+----+\n",
      "|          hs_ID|month|year|fin_out|hs_unt|hs_tel|else_tel|tel_intv|fam_inc|hs_type|intv_type|REG|DIV|race|\n",
      "+---------------+-----+----+-------+------+------+--------+--------+-------+-------+---------+---+---+----+\n",
      "|000004795110719|   12|2017|    201|     1|     1|       1|       1|      9|      1|        2|  3|  6|   1|\n",
      "|000004795110719|   12|2017|    201|     1|     1|       1|       1|      9|      1|        2|  3|  6|   1|\n",
      "|000071691004941|   12|2017|    201|     1|     1|       1|       1|     11|      1|        1|  3|  6|   1|\n",
      "|000071691004941|   12|2017|    201|     1|     1|       1|       1|     11|      1|        1|  3|  6|   1|\n",
      "|000071691004941|   12|2017|    201|     1|     1|       1|       1|     11|      1|        1|  3|  6|   1|\n",
      "|000110177987986|   12|2017|    201|     1|     1|       1|       1|     14|      1|        1|  3|  6|   2|\n",
      "|000110177987986|   12|2017|    201|     1|     1|       1|       1|     14|      1|        1|  3|  6|   2|\n",
      "|000110206593381|   12|2017|    213|     1|     1|       1|       0|     -1|      0|        1|  3|  6|  -1|\n",
      "|000110284815680|   12|2017|    201|     1|     1|       1|       1|      9|      7|        1|  3|  6|   1|\n",
      "|000110327856469|   12|2017|    201|     1|     1|       1|       1|      5|      7|        1|  3|  6|   2|\n",
      "+---------------+-----+----+-------+------+------+--------+--------+-------+-------+---------+---+---+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Define the schema based on the updated column descriptions\n",
    "schema = StructType([\n",
    "    StructField(\"hs_ID\", StringType(), False),\n",
    "    StructField(\"month\", IntegerType(), False),\n",
    "    StructField(\"year\", IntegerType(), False),\n",
    "    StructField(\"fin_out\", StringType(), False),\n",
    "    StructField(\"hs_unt\", IntegerType(), False),\n",
    "    StructField(\"hs_tel\", IntegerType(), False),\n",
    "    StructField(\"else_tel\", IntegerType(), False),\n",
    "    StructField(\"tel_intv\", IntegerType(), False),\n",
    "    StructField(\"fam_inc\", IntegerType(), False),\n",
    "    StructField(\"hs_type\", IntegerType(), False),\n",
    "    StructField(\"intv_type\", IntegerType(), False), \n",
    "    StructField(\"REG\", IntegerType(), False),\n",
    "    StructField(\"DIV\", StringType(), False),\n",
    "    StructField(\"race\", StringType(), False)\n",
    "    # Add more fields according to your actual schema\n",
    "])\n",
    "\n",
    "# Define a UDF to extract values from text rows\n",
    "def extract_values(text_row):\n",
    "    values = [\n",
    "        text_row[0][0:15],  # HRHHID\n",
    "        int(text_row[0][15:17]),  # HRMONTH\n",
    "        int(text_row[0][17:21]),  # HRYEAR4\n",
    "        text_row[0][23:26],  # HUFINAL\n",
    "        int(text_row[0][30:32]),  # HEHOUSUT\n",
    "        int(text_row[0][33:34]),  # HETELHHD\n",
    "        int(text_row[0][35:36]),  # HETELAVL\n",
    "        int(text_row[0][37:38]),  # HEPHONEO\n",
    "        int(text_row[0][38:40]),  # HEFAMINC\n",
    "        int(text_row[0][60:62]),  # HRHTYPE \n",
    "        int(text_row[0][65:66]),  # HUINTTYP\n",
    "        int(text_row[0][88:90]),  #REG\n",
    "        text_row[0][90:91],       #DIV\n",
    "        text_row[0][138:140]  # PTDTRACE\n",
    "        # Add more value extractions based on your actual schema\n",
    "    ]\n",
    "    \n",
    "    return values\n",
    "\n",
    "# Register the UDF\n",
    "extract_udf = udf(extract_values, schema)\n",
    "# Assuming your preprocessed_data DataFrame has a column named \"text_row\"\n",
    "processed_data = processed_data.withColumn(\"extracted_values\", extract_udf(col(\"columns\")))\n",
    "\n",
    "# Define column names\n",
    "# Define column names\n",
    "column_names = [\"hs_ID\", \"month\", \"year\", \"fin_out\", \"hs_unt\",\n",
    "                \"hs_tel\", \"else_tel\", \"tel_intv\", \"fam_inc\", \"hs_type\", \"intv_type\", \"REG\", \"DIV\",\n",
    "                \"race\"]\n",
    "\n",
    "# Select the extracted values and alias the columns\n",
    "selected_df = processed_data.select(*[col(\"extracted_values\")[col_name].alias(col_name) for col_name in column_names])\n",
    "# Create a new column \"house_tel\" using values from the dictionary\n",
    "\n",
    "\n",
    "print(selected_df)\n",
    "# Show the resulting DataFrame\n",
    "selected_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be0adb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----+-------+--------------------+------+--------+--------+---------+-------+-------+---+---+----+\n",
      "|          hs_ID|month|year|fin_out|              hs_unt|hs_tel|else_tel|tel_intv|intv_type|fam_inc|hs_type|REG|DIV|race|\n",
      "+---------------+-----+----+-------+--------------------+------+--------+--------+---------+-------+-------+---+---+----+\n",
      "|000004795110719|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|       NO|      9|      1|  3|  6|   1|\n",
      "|000004795110719|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|       NO|      9|      1|  3|  6|   1|\n",
      "|000071691004941|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     11|      1|  3|  6|   1|\n",
      "|000071691004941|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     11|      1|  3|  6|   1|\n",
      "|000071691004941|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     11|      1|  3|  6|   1|\n",
      "|000110177987986|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     14|      1|  3|  6|   2|\n",
      "|000110177987986|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     14|      1|  3|  6|   2|\n",
      "|000110206593381|   12|2017|    213|HOUSE, APARTMENT,...|   YES|     YES|      NO|      YES|     -1|      0|  3|  6|  -1|\n",
      "|000110284815680|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|      9|      7|  3|  6|   1|\n",
      "|000110327856469|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|      5|      7|  3|  6|   2|\n",
      "|000110339935453|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|       NO|     11|      4|  3|  6|   2|\n",
      "|000110339935453|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|       NO|     11|      4|  3|  6|   2|\n",
      "|000110339935453|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|       NO|     11|      4|  3|  6|   2|\n",
      "|000110339935453|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|       NO|     11|      4|  3|  6|   2|\n",
      "|000110343868567|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|      NO|      YES|     14|      1|  3|  6|   1|\n",
      "|000110343868567|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|      NO|      YES|     14|      1|  3|  6|   1|\n",
      "|000110359424339|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     13|      4|  3|  6|   2|\n",
      "|000110359424339|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     13|      4|  3|  6|   2|\n",
      "|000110415400429|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     10|      1|  3|  6|   1|\n",
      "|000110415400429|   12|2017|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     10|      1|  3|  6|   1|\n",
      "+---------------+-----+----+-------+--------------------+------+--------+--------+---------+-------+-------+---+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, create_map, lit\n",
    "\n",
    "# Define mappings\n",
    "hs_unt_mapping = {0: \"OTHER UNIT\", 1: \"HOUSE, APARTMENT, FLAT\"}\n",
    "hs_tel_mapping = {1: \"YES\", 2: \"NO\"}\n",
    "else_tel_mapping = {1: \"YES\", 2: \"NO\"}\n",
    "tel_intv_mapping = {1: \"YES\", 0: \"NO\"}\n",
    "\n",
    "intv_type_mapping = {1: \"YES\", 2: \"NO\"}\n",
    "\n",
    "# Apply mappings using the when function\n",
    "mapped_df = selected_df.select(\n",
    "    \"hs_ID\",\n",
    "    \"month\",\n",
    "    \"year\",\n",
    "    \"fin_out\",\n",
    "    when(col(\"hs_unt\") == 0, \"OTHER UNIT\").otherwise(\"HOUSE, APARTMENT, FLAT\").alias(\"hs_unt\"),\n",
    "    when(col(\"hs_tel\") == 1, \"YES\").otherwise(\"NO\").alias(\"hs_tel\"),\n",
    "    when(col(\"else_tel\") == 1, \"YES\").otherwise(\"NO\").alias(\"else_tel\"),\n",
    "    when(col(\"tel_intv\") == 1, \"YES\").otherwise(\"NO\").alias(\"tel_intv\"),\n",
    "    #fam_income_map[col(\"fam_income\")].alias(\"fam_income\"),\n",
    "    #hs_type_map[col(\"hs_type\")].alias(\"hs_type\"),\n",
    "    when(col(\"intv_type\") == 1, \"YES\").otherwise(\"NO\").alias(\"intv_type\"),\n",
    "    #region_map[col(\"REG\")].alias(\"REG\"),\n",
    "    #division_map[col(\"DIV\")].alias(\"DIV\"),\n",
    "    \"fam_inc\",\n",
    "    \"hs_type\",\n",
    "    \"REG\",\n",
    "    \"DIV\",\n",
    "    \"race\"\n",
    ")\n",
    "\n",
    "# Show the mapped DataFrame\n",
    "mapped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2b03cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+--------------------+------+--------+--------+---------+-------+-------+----+---------+-------+\n",
      "|          hs_ID|fin_out|              hs_unt|hs_tel|else_tel|tel_intv|intv_type|fam_inc|hs_type|race|intv_date|reg/div|\n",
      "+---------------+-------+--------------------+------+--------+--------+---------+-------+-------+----+---------+-------+\n",
      "|000004795110719|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|       NO|      9|      1|   1| 2017/Dec|    3/6|\n",
      "|000004795110719|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|       NO|      9|      1|   1| 2017/Dec|    3/6|\n",
      "|000071691004941|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     11|      1|   1| 2017/Dec|    3/6|\n",
      "|000071691004941|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     11|      1|   1| 2017/Dec|    3/6|\n",
      "|000071691004941|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     11|      1|   1| 2017/Dec|    3/6|\n",
      "|000110177987986|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     14|      1|   2| 2017/Dec|    3/6|\n",
      "|000110177987986|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|     14|      1|   2| 2017/Dec|    3/6|\n",
      "|000110206593381|    213|HOUSE, APARTMENT,...|   YES|     YES|      NO|      YES|     -1|      0|  -1| 2017/Dec|    3/6|\n",
      "|000110284815680|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|      9|      7|   1| 2017/Dec|    3/6|\n",
      "|000110327856469|    201|HOUSE, APARTMENT,...|   YES|     YES|     YES|      YES|      5|      7|   2| 2017/Dec|    3/6|\n",
      "+---------------+-------+--------------------+------+--------+--------+---------+-------+-------+----+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat, expr, lit\n",
    "import calendar\n",
    "\n",
    "# Add a new column with concatenated year and month in \"YYYY/MMM\" format\n",
    "intv_time = mapped_df.withColumn(\n",
    "    \"intv_date\",\n",
    "    concat(\n",
    "        col(\"year\"), lit(\"/\"), \n",
    "        expr(\"substring('JanFebMarAprMayJunJulAugSepOctNovDec', (month - 1) * 3 + 1, 3)\")\n",
    "    )\n",
    ")\n",
    "\n",
    "new_df = intv_time.withColumn(\"reg/div\", concat(\"REG\", lit(\"/\"), \"DIV\"))\n",
    "\n",
    "# Drop year and month columns\n",
    "new_df = new_df.drop(\"year\", \"month\", \"REG\", \"DIV\")\n",
    "# Show the resulting DataFrame\n",
    "new_df.show(10)\n",
    "\n",
    "# Write the DataFrame to a CSV file with overwrite mode\n",
    "new_df.write.mode(\"overwrite\").csv(\"path/to/output/folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b98559",
   "metadata": {},
   "source": [
    "## QUESTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382bb5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|fam_inc|count|\n",
      "+-------+-----+\n",
      "|     -1|20391|\n",
      "|     12| 9971|\n",
      "|      1| 3136|\n",
      "|     13|13442|\n",
      "|      6| 4518|\n",
      "|     16|15704|\n",
      "|      3| 2277|\n",
      "|      5| 2614|\n",
      "|     15|17794|\n",
      "|      9| 6743|\n",
      "|      4| 3161|\n",
      "|      8| 5803|\n",
      "|      7| 6312|\n",
      "|     10| 6620|\n",
      "|     11| 9788|\n",
      "|     14|16557|\n",
      "|      2| 1625|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by \"fam_income\" and calculate the count of responders\n",
    "family_income_counts = new_df.groupBy(\"fam_inc\").count()\n",
    "# Show the resulting DataFrame\n",
    "family_income_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8aaf691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|fam_inc|\n",
      "+-------+\n",
      "|     -1|\n",
      "|     12|\n",
      "|      1|\n",
      "|     13|\n",
      "|      6|\n",
      "|     16|\n",
      "|      3|\n",
      "|      5|\n",
      "|     15|\n",
      "|      9|\n",
      "|      4|\n",
      "|      8|\n",
      "|      7|\n",
      "|     10|\n",
      "|     11|\n",
      "|     14|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_values = new_df.select(\"fam_inc\").distinct()\n",
    "# Show the unique values\n",
    "unique_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3a661",
   "metadata": {},
   "source": [
    "Where;\n",
    "-1 = Outlier\n",
    "\n",
    "1 = LESS THAN $5,000\n",
    "\n",
    "2 = 5,000 TO 7,499\n",
    "\n",
    "3 = 7,500 TO 9,999\n",
    "\n",
    "4 = 10,000 TO 12,499\n",
    "\n",
    "5 = 12,500 TO 14,999\n",
    "\n",
    "6 = 15,000 TO 19,999\n",
    "\n",
    "7 = 20,000 TO 24,999\n",
    "\n",
    "8 = 25,000 TO 29,999\n",
    "\n",
    "9 = 30,000 TO 34,999\n",
    "\n",
    "10 = 35,000 TO 39,999\n",
    "\n",
    "11 = 40,000 TO 49,999\n",
    "\n",
    "12  = 50,000 TO 59,999\n",
    "\n",
    "13 = 60,000 TO 74,999\n",
    "\n",
    "14 = 75,000 TO 99,999\n",
    "\n",
    "15 = 100,000 TO 149,999\n",
    "\n",
    "16 = 150,000 OR MORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab129a9",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb703ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------------+\n",
      "|reg/div|race|responder_count|\n",
      "+-------+----+---------------+\n",
      "|    3/5|   1|          16999|\n",
      "|    4/8|   1|          14343|\n",
      "|    4/9|   1|          13214|\n",
      "|    2/3|   1|          11325|\n",
      "|    3/7|   1|          11248|\n",
      "|    2/4|   1|           9884|\n",
      "|    1/2|   1|           8487|\n",
      "|    1/1|   1|           8410|\n",
      "|    3/6|   1|           6580|\n",
      "|    3/5|   2|           4899|\n",
      "+-------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by \"reg_div_concat\" and \"race\" and calculate the count of responders\n",
    "grouped_df = new_df.groupBy(\"reg/div\", \"race\").agg(F.count(\"*\").alias(\"responder_count\"))\n",
    "\n",
    "# Order the DataFrame by \"responder_count\" in descending order\n",
    "ordered_df = grouped_df.orderBy(F.desc(\"responder_count\"))\n",
    "\n",
    "# Select the top 10 rows\n",
    "top_10_responder_counts = ordered_df.limit(10)\n",
    "\n",
    "# Show the top 10 rows\n",
    "top_10_responder_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2310294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|reg/div|\n",
      "+-------+\n",
      "|    3/7|\n",
      "|    2/4|\n",
      "|    3/6|\n",
      "|    3/5|\n",
      "|    2/3|\n",
      "|    4/9|\n",
      "|    1/2|\n",
      "|    4/8|\n",
      "|    1/1|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_location = new_df.select(\"reg/div\").distinct()\n",
    "# Show the unique values\n",
    "unique_location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e4c3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|race|\n",
      "+----+\n",
      "|   1|\n",
      "|   2|\n",
      "|   3|\n",
      "|   4|\n",
      "|   5|\n",
      "|   6|\n",
      "|   7|\n",
      "|   8|\n",
      "|   9|\n",
      "|  -1|\n",
      "|  10|\n",
      "|  11|\n",
      "|  12|\n",
      "|  13|\n",
      "|  14|\n",
      "|  15|\n",
      "|  16|\n",
      "|  17|\n",
      "|  18|\n",
      "|  19|\n",
      "|  20|\n",
      "|  21|\n",
      "|  23|\n",
      "|  24|\n",
      "|  25|\n",
      "|  26|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_race = new_df.select(\"race\").distinct().sort(\"race\")\n",
    "# Show the unique values\n",
    "unique_race.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca55da",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "606f5e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of responders: 636\n"
     ]
    }
   ],
   "source": [
    "# Assuming \"df\" is your DataFrame\n",
    "filtered_df = new_df.filter(\n",
    "    (new_df.hs_tel == \"NO\")  # No telephone access in the house\n",
    "    & (new_df.else_tel == \"YES\")  # Can access telephone elsewhere\n",
    "    & (new_df.tel_intv == \"YES\")  # Telephone interview accepted\n",
    ")\n",
    "\n",
    "# Count the responders matching the criteria\n",
    "count = filtered_df.count()\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of responders:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad7bd5",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ce2964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of responders: 25795\n"
     ]
    }
   ],
   "source": [
    "# Assuming \"df\" is your DataFrame\n",
    "new_filtered_df = new_df.filter(\n",
    "    ((new_df.hs_tel == \"YES\") | (new_df.else_tel == \"YES\"))  # Telephone access in the house or elsewhere\n",
    "    & (new_df.tel_intv == \"NO\")  # Telephone interview not accepted\n",
    ")\n",
    "\n",
    "# Count the responders matching the criteria\n",
    "count = new_filtered_df.count()\n",
    "\n",
    "# Show the result\n",
    "print(\"Number of responders:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9a66ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tel_intv|\n",
      "+--------+\n",
      "|     YES|\n",
      "|      NO|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_tel_intv = new_df.select(\"tel_intv\").distinct()\n",
    "# Show the unique values\n",
    "unique_tel_intv.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
